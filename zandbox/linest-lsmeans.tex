%\VignetteEngine{knitr::knitr} 
%\VignettePackage{doBy}
%\VignetteIndexEntry{doBy: Linear estimates and LSmeans}
%\VignetteIndexEntry{LSMEANS}
%\VignetteIndexEntry{contrasts}
%\VignetteIndexEntry{estimable functions}

\documentclass[11pt]{article}\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

\usepackage{hyperref,color,boxedminipage,Sweave,a4wide}
\usepackage[utf8]{inputenc}
\usepackage{url,hyperref}
\usepackage{boxedminipage,color}

\usepackage[authoryear,round]{natbib}
\bibliographystyle{plainnat}

\RequirePackage{color,fancyvrb,amsmath,amsfonts}


%% \usepackage[utf8]{inputenc}
%% \usepackage[inline,nomargin,draft]{fixme}
%% \newcommand\FXInline[2]{\textbf{\color{blue}#1}:
%%   \emph{\color{blue} - #2}}
%% \usepackage[cm]{fullpage}


\def\R{\texttt{R}}
\def\pkg#1{{\bf #1}}
\def\doby{\pkg{doBy}}

\def\code#1{\texttt{#1}}
\def\cc#1{\texttt{#1}}
\def\comi#1{{\it #1}}
\def\esticon{\code{esticon()}}
\def\lsmeans{\code{LSmeans}}
\def\linmat{\code{LE_matrix()}}
\def\linest{\code{linest()}}

% reduce whitespace between R code and R output
\let\oldknitrout\knitrout
\renewenvironment{knitrout}{
  \begin{oldknitrout}
    \footnotesize
    \topsep=0pt
}{
  \end{oldknitrout}
}

% show R> prompt before R commands


\DeclareMathOperator{\EE}{\mathbb{E}}
\DeclareMathOperator{\var}{\mathbb{V}ar}
\DeclareMathOperator{\cov}{\mathbb{C}ov}
\DeclareMathOperator{\norm}{N}
\DeclareMathOperator{\spanx}{span}
\DeclareMathOperator{\corr}{Corr}
\DeclareMathOperator{\deter}{det}
\DeclareMathOperator{\trace}{tr}
\def\inv{^{-1}}
\newcommand{\transp}{^{\top}}







\title{Linear estimates and LS--means in the \texttt{doBy} package}
\author{S{\o}ren H{\o}jsgaard and Ulrich Halekoh}
\date{\pkg{doBy} version 4.6.11 as of 2021-11-29}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
%%\SweaveOpts{concordance=TRUE}




\maketitle
\hrule
\tableofcontents

\parindent0pt
\parskip5pt




%\tableofcontents
%% \setkeys{Gin}{height=3.5in, width=3.5in}



%% \begin{quote}
%%   This is a draft; please feel free to suggest improvements.
%% \end{quote}

\section{Introduction}
\label{sec:introduction}

\subsection{Linear functions of parameters}
\label{sec:line-funct-param}

A linear function of a $p$--dimensional parameter vector $\beta$ has
the form
\begin{displaymath}
  C=L\beta
\end{displaymath}
where $L$ is a $q\times p$ matrix which we call the \comi{Linear
  Estimate Matrix} of simply \comi{LE-matrix}.  The corresponding
linear estimate is $\hat C = L \hat \beta$.  A linear hypothesis has
the form $H_0: L\beta=m$ for some $q$ dimensional vector $m$.

\subsection{Tooth growth}
\label{sec:tooth-growth}

The response is the length of odontoblasts cells (cells responsible for
tooth growth) in 60 guinea pigs.  Each animal received one of
three dose levels of vitamin C (0.5, 1, and 2 mg/day) by one of
two delivery methods, (orange juice (coded as OJ) or ascorbic acid 
(a form of vitamin C and (coded as VC)).

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlkwd{head}\hlstd{(ToothGrowth,} \hlnum{4}\hlstd{)}
\end{alltt}
\begin{verbatim}
##    len supp dose
## 1  4.2   VC  0.5
## 2 11.5   VC  0.5
## 3  7.3   VC  0.5
## 4  5.8   VC  0.5
\end{verbatim}
\begin{alltt}
\hlstd{> }\hlkwd{ftable}\hlstd{(}\hlkwd{xtabs}\hlstd{(}\hlopt{~} \hlstd{dose} \hlopt{+} \hlstd{supp,} \hlkwc{data}\hlstd{=ToothGrowth))}
\end{alltt}
\begin{verbatim}
##      supp OJ VC
## dose           
## 0.5       10 10
## 1         10 10
## 2         10 10
\end{verbatim}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}
\includegraphics[width=\maxwidth]{figures/LSmeansunnamed-chunk-5-1} \caption[Plot of length against dose for difference sources of vitamin C]{Plot of length against dose for difference sources of vitamin C.}\label{fig:unnamed-chunk-5}
\end{figure}

\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{ToothGrowth} \hlopt{%>%} \hlkwd{interaction_plot}\hlstd{(len} \hlopt{~} \hlstd{dose} \hlopt{+} \hlstd{supp)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figures/LSmeansunnamed-chunk-6-1} 
\end{knitrout}

The interaction plot suggests a mild interaction which is supported by a formal comparison:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{ToothGrowth}\hlopt{$}\hlstd{dose} \hlkwb{<-} \hlkwd{factor}\hlstd{(ToothGrowth}\hlopt{$}\hlstd{dose)}
\hlstd{> }\hlkwd{head}\hlstd{(ToothGrowth)}
\end{alltt}
\begin{verbatim}
##    len supp dose
## 1  4.2   VC  0.5
## 2 11.5   VC  0.5
## 3  7.3   VC  0.5
## 4  5.8   VC  0.5
## 5  6.4   VC  0.5
## 6 10.0   VC  0.5
\end{verbatim}
\begin{alltt}
\hlstd{> }\hlstd{tooth1} \hlkwb{<-} \hlkwd{lm}\hlstd{(len} \hlopt{~} \hlstd{dose} \hlopt{+} \hlstd{supp,} \hlkwc{data}\hlstd{=ToothGrowth)}
\hlstd{> }\hlstd{tooth2} \hlkwb{<-} \hlkwd{lm}\hlstd{(len} \hlopt{~} \hlstd{dose} \hlopt{*} \hlstd{supp,} \hlkwc{data}\hlstd{=ToothGrowth)}
\hlstd{> }\hlkwd{anova}\hlstd{(tooth1, tooth2)}
\end{alltt}
\begin{verbatim}
## Analysis of Variance Table
## 
## Model 1: len ~ dose + supp
## Model 2: len ~ dose * supp
##   Res.Df RSS Df Sum of Sq    F Pr(>F)  
## 1     56 820                           
## 2     54 712  2       108 4.11  0.022 *
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}
\end{kframe}
\end{knitrout}


%% <<linear:interaction, fig.cap="Interaction plot for the ToothGrowth data. The average length for each group is a dot. Boxplot outliers are crosses." >>= 
%% tooth_avglen <- ToothGrowth %>%
%%   group_by(dose, supp) %>%
%%   summarise(val = mean(len))
%% tooth_avglen
%% ggplot(ToothGrowth, aes(x = factor(dose), y = len, colour = supp)) +
%%     geom_boxplot(outlier.shape = 4) +
%%     geom_point(data = tooth_avglen, aes(y = val)) +
%%     geom_line(data = tooth_avglen, aes(y = val, group = supp))
%%@


\section{Computing linear estimates}
\label{sec:comp-line-estim}


For now, we focus on the additive model:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{tooth1}
\end{alltt}
\begin{verbatim}
## 
## Call:
## lm(formula = len ~ dose + supp, data = ToothGrowth)
## 
## Coefficients:
## (Intercept)        dose1        dose2       suppVC  
##       12.46         9.13        15.49        -3.70
\end{verbatim}
\end{kframe}
\end{knitrout}

Consider computing the estimated length for each dose of orange juice (OJ):
One option: Construct the LE--matrix $L$ directly:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{L} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{0}\hlstd{,}
\hlstd{  }              \hlnum{1}\hlstd{,} \hlnum{1}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{0}\hlstd{,}
\hlstd{  }              \hlnum{1}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{1}\hlstd{,} \hlnum{0}\hlstd{),} \hlkwc{nrow}\hlstd{=}\hlnum{3}\hlstd{,} \hlkwc{byrow}\hlstd{=T)}
\end{alltt}
\end{kframe}
\end{knitrout}

Then do:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{c1} \hlkwb{<-} \hlkwd{linest}\hlstd{(tooth1, L)}
\hlstd{> }\hlstd{c1}
\end{alltt}
\begin{verbatim}
## Coefficients:
##      estimate std.error statistic     df p.value
## [1,]   12.455     0.988    12.603 56.000       0
## [2,]   21.585     0.988    21.841 56.000       0
## [3,]   27.950     0.988    28.281 56.000       0
\end{verbatim}
\end{kframe}
\end{knitrout}

We can do:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlkwd{summary}\hlstd{(c1)}
\end{alltt}
\begin{verbatim}
## Coefficients:
##      estimate std.error statistic     df p.value
## [1,]   12.455     0.988    12.603 56.000       0
## [2,]   21.585     0.988    21.841 56.000       0
## [3,]   27.950     0.988    28.281 56.000       0
## 
## Grid:
## NULL
## 
## L:
##      [,1] [,2] [,3] [,4]
## [1,]    1    0    0    0
## [2,]    1    1    0    0
## [3,]    1    0    1    0
\end{verbatim}
\begin{alltt}
\hlstd{> }\hlkwd{coef}\hlstd{(c1)}
\end{alltt}
\begin{verbatim}
##   estimate std.error statistic df   p.value
## 1    12.46    0.9883     12.60 56 5.490e-18
## 2    21.59    0.9883     21.84 56 4.461e-29
## 3    27.95    0.9883     28.28 56 7.627e-35
\end{verbatim}
\begin{alltt}
\hlstd{> }\hlkwd{confint}\hlstd{(c1)}
\end{alltt}
\begin{verbatim}
##   0.025 0.975
## 1 10.48 14.43
## 2 19.61 23.56
## 3 25.97 29.93
\end{verbatim}
\end{kframe}
\end{knitrout}

\subsection{Automatic generation of $L$}
\label{sec:autom-gener-l}


The matrix $L$ can be generated as follows:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{L} \hlkwb{<-} \hlkwd{LE_matrix}\hlstd{(tooth1,} \hlkwc{effect}\hlstd{=}\hlstr{"dose"}\hlstd{,} \hlkwc{at}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{supp}\hlstd{=}\hlstr{"OJ"}\hlstd{))}
\hlstd{> }\hlstd{L}
\end{alltt}
\begin{verbatim}
##      (Intercept) dose1 dose2 suppVC
## [1,]           1     0     0      0
## [2,]           1     1     0      0
## [3,]           1     0     1      0
\end{verbatim}
\end{kframe}
\end{knitrout}


\subsection{Alternatives - \texttt{esticon()}}
\label{sec:alternatives}


An alternative is to do:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{c1} \hlkwb{<-} \hlkwd{esticon}\hlstd{(tooth1, L)}
\hlstd{> }\hlstd{c1}
\end{alltt}
\begin{verbatim}
##      estimate std.error statistic p.value  beta0 df
## [1,]   12.455     0.988    12.603   0.000  0.000 56
## [2,]   21.585     0.988    21.841   0.000  0.000 56
## [3,]   27.950     0.988    28.281   0.000  0.000 56
\end{verbatim}
\end{kframe}
\end{knitrout}

Notice: \verb|esticon| has been in the \doby\ package for many years;
\verb|linest| is a newer addition; \verb|esticon| is not actively
maintained but remains in \doby\ for historical reasons.  Yet another
alternative in this case is to generate a new data frame and then
invoke predict (but this approach is not generally applicable, see later):

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{nd} \hlkwb{<-} \hlkwd{data.frame}\hlstd{(}\hlkwc{dose}\hlstd{=}\hlkwd{c}\hlstd{(}\hlstr{'0.5'}\hlstd{,} \hlstr{'1'}\hlstd{,} \hlstr{'2'}\hlstd{),} \hlkwc{supp}\hlstd{=}\hlstr{'OJ'}\hlstd{)}
\hlstd{> }\hlstd{nd}
\end{alltt}
\begin{verbatim}
##   dose supp
## 1  0.5   OJ
## 2    1   OJ
## 3    2   OJ
\end{verbatim}
\begin{alltt}
\hlstd{> }\hlkwd{predict}\hlstd{(tooth1,} \hlkwc{newdata}\hlstd{=nd)}
\end{alltt}
\begin{verbatim}
##     1     2     3 
## 12.46 21.59 27.95
\end{verbatim}
\end{kframe}
\end{knitrout}


\section{Least-squares means (LS--means)}
\label{sec:least-squares-means}


A related question could be: What is the estimated length for each
dose if we ignore the source of vitamin C (i.e.\ whether it is OJ or
VC). One approach would be to fit a model in which source does not appear:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{tooth0} \hlkwb{<-} \hlkwd{update}\hlstd{(tooth1, .} \hlopt{~} \hlstd{.} \hlopt{-} \hlstd{supp)}
\hlstd{> }\hlstd{L0} \hlkwb{<-} \hlkwd{LE_matrix}\hlstd{(tooth0,} \hlkwc{effect}\hlstd{=}\hlstr{"dose"}\hlstd{)}
\hlstd{> }\hlstd{L0}
\end{alltt}
\begin{verbatim}
##      (Intercept) dose1 dose2
## [1,]           1     0     0
## [2,]           1     1     0
## [3,]           1     0     1
\end{verbatim}
\begin{alltt}
\hlstd{> }\hlkwd{linest}\hlstd{(tooth0,} \hlkwc{L}\hlstd{=L0)}
\end{alltt}
\begin{verbatim}
## Coefficients:
##      estimate std.error statistic     df p.value
## [1,]   10.605     0.949    11.180 57.000       0
## [2,]   19.735     0.949    20.805 57.000       0
## [3,]   26.100     0.949    27.515 57.000       0
\end{verbatim}
\end{kframe}
\end{knitrout}

An alternative would be to stick to the original model but compute the
estimate for an ``average vitamin C source''. That would correspond to
giving weight $1/2$ to each of the two vitamin C source
parameters. However, as one of the parameters is already set to zero
to obtain identifiability, we obtain the LE--matrix $L$ as

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{L1} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{0.5}\hlstd{,}
\hlstd{  }               \hlnum{1}\hlstd{,} \hlnum{1}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{0.5}\hlstd{,}
\hlstd{  }               \hlnum{1}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{1}\hlstd{,} \hlnum{0.5}\hlstd{),} \hlkwc{nrow}\hlstd{=}\hlnum{3}\hlstd{,} \hlkwc{byrow}\hlstd{=T)}
\hlstd{> }\hlkwd{linest}\hlstd{(tooth1,} \hlkwc{L}\hlstd{=L1)}
\end{alltt}
\begin{verbatim}
## Coefficients:
##      estimate std.error statistic     df p.value
## [1,]   10.605     0.856    12.391 56.000       0
## [2,]   19.735     0.856    23.058 56.000       0
## [3,]   26.100     0.856    30.495 56.000       0
\end{verbatim}
\end{kframe}
\end{knitrout}

Such a particular linear estimate is sometimes called a least-squares
mean or an LSmean or a marginal mean. Notice that the parameter
estimates under the two approaches are identical. This is is because
data is balanced: There are $10$ observations per supplementation
type. Had data not been balanced, the estimates would in general have been different. 

Notice: One may generate $L$ automatically with
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{L1} \hlkwb{<-} \hlkwd{LE_matrix}\hlstd{(tooth1,} \hlkwc{effect}\hlstd{=}\hlstr{"dose"}\hlstd{)}
\hlstd{> }\hlstd{L1}
\end{alltt}
\begin{verbatim}
##      (Intercept) dose1 dose2 suppVC
## [1,]           1     0     0    0.5
## [2,]           1     1     0    0.5
## [3,]           1     0     1    0.5
\end{verbatim}
\end{kframe}
\end{knitrout}

Notice: One may obtain the LSmean directly as:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlkwd{LSmeans}\hlstd{(tooth1,} \hlkwc{effect}\hlstd{=}\hlstr{"dose"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## Coefficients:
##      estimate std.error statistic     df p.value
## [1,]   10.605     0.856    12.391 56.000       0
## [2,]   19.735     0.856    23.058 56.000       0
## [3,]   26.100     0.856    30.495 56.000       0
\end{verbatim}
\end{kframe}
\end{knitrout}

%% \subsection{Additive model}
%% \label{sec:additive-model}

%% Returning to the \verb|ToothGrowth| data, orange juice and ascorbic
%% acid are just two of many ways of supplying vitamin C (citrus and lime
%% juice would be two alternatives). Here one can therefore argue, that
%% it would make sense to estimate the the effect for each dose for an
%% ``average vitamin C source'':

%% << >>= 
%% LSmeans(tooth1, effect="dose")
%% @

which is the same as

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{L} \hlkwb{<-} \hlkwd{LE_matrix}\hlstd{(tooth1,} \hlkwc{effect}\hlstd{=}\hlstr{"dose"}\hlstd{)}
\hlstd{> }\hlstd{le} \hlkwb{<-} \hlkwd{linest}\hlstd{(tooth1,} \hlkwc{L}\hlstd{=L)}
\hlstd{> }\hlkwd{coef}\hlstd{(le)}
\end{alltt}
\end{kframe}
\end{knitrout}

%% \subsection{Interaction model}
%% \label{sec:interaction-model}


For a model with interactions, the LSmeans are
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlkwd{LSmeans}\hlstd{(tooth2,} \hlkwc{effect}\hlstd{=}\hlstr{"dose"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## Coefficients:
##      estimate std.error statistic     df p.value
## [1,]   10.605     0.812    13.060 54.000       0
## [2,]   19.735     0.812    24.304 54.000       0
## [3,]   26.100     0.812    32.143 54.000       0
\end{verbatim}
\end{kframe}
\end{knitrout}

In this case, the LE--matrix is
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{L} \hlkwb{<-} \hlkwd{LE_matrix}\hlstd{(tooth2,} \hlkwc{effect}\hlstd{=}\hlstr{"dose"}\hlstd{)}
\hlstd{> }\hlkwd{t}\hlstd{(L)}
\end{alltt}
\begin{verbatim}
##              [,1] [,2] [,3]
## (Intercept)   1.0  1.0  1.0
## dose1         0.0  1.0  0.0
## dose2         0.0  0.0  1.0
## suppVC        0.5  0.5  0.5
## dose1:suppVC  0.0  0.5  0.0
## dose2:suppVC  0.0  0.0  0.5
\end{verbatim}
\end{kframe}
\end{knitrout}


\subsection{Using the \code{at=} argument}
\label{sec:at-argument}


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlkwd{library}\hlstd{(ggplot2)}
\hlstd{> }\hlstd{ChickWeight}\hlopt{$}\hlstd{Diet} \hlkwb{<-} \hlkwd{factor}\hlstd{(ChickWeight}\hlopt{$}\hlstd{Diet)}
\hlstd{> }\hlkwd{qplot}\hlstd{(Time, weight,} \hlkwc{data}\hlstd{=ChickWeight,} \hlkwc{colour}\hlstd{=Chick,} \hlkwc{facets}\hlstd{=}\hlopt{~}\hlstd{Diet,}
\hlstd{  }      \hlkwc{geom}\hlstd{=}\hlkwd{c}\hlstd{(}\hlstr{"point"}\hlstd{,}\hlstr{"line"}\hlstd{))} \hlopt{+} \hlkwd{theme}\hlstd{(}\hlkwc{legend.position}\hlstd{=}\hlstr{"none"}\hlstd{)}
\end{alltt}
\end{kframe}\begin{figure}
\includegraphics[width=\maxwidth]{figures/LSmeanschick-fig-1} \caption[ChickWeight data]{ChickWeight data.}\label{fig:chick-fig}
\end{figure}

\end{knitrout}

Consider random regression model:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlkwd{library}\hlstd{(lme4)}
\hlstd{> }\hlstd{chick} \hlkwb{<-} \hlkwd{lmer}\hlstd{(weight} \hlopt{~} \hlstd{Time} \hlopt{*} \hlstd{Diet} \hlopt{+} \hlstd{(}\hlnum{0} \hlopt{+} \hlstd{Time} \hlopt{|} \hlstd{Chick),}
\hlstd{  }           \hlkwc{data}\hlstd{=ChickWeight)}
\hlstd{> }\hlkwd{coef}\hlstd{(}\hlkwd{summary}\hlstd{(chick))}
\end{alltt}
\begin{verbatim}
##             Estimate Std. Error t value
## (Intercept)   33.218     1.7697 18.7701
## Time           6.339     0.6103 10.3855
## Diet2         -4.585     3.0047 -1.5258
## Diet3        -14.968     3.0047 -4.9815
## Diet4         -1.454     3.0177 -0.4818
## Time:Diet2     2.271     1.0367  2.1902
## Time:Diet3     5.084     1.0367  4.9043
## Time:Diet4     3.217     1.0377  3.1004
\end{verbatim}
\end{kframe}
\end{knitrout}


The LE--matrix for \cc{Diet} becomes:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{L} \hlkwb{<-} \hlkwd{LE_matrix}\hlstd{(chick,} \hlkwc{effect}\hlstd{=}\hlstr{"Diet"}\hlstd{)}
\hlstd{> }\hlkwd{t}\hlstd{(L)}
\end{alltt}
\begin{verbatim}
##              [,1]  [,2]  [,3]  [,4]
## (Intercept)  1.00  1.00  1.00  1.00
## Time        10.72 10.72 10.72 10.72
## Diet2        0.00  1.00  0.00  0.00
## Diet3        0.00  0.00  1.00  0.00
## Diet4        0.00  0.00  0.00  1.00
## Time:Diet2   0.00 10.72  0.00  0.00
## Time:Diet3   0.00  0.00 10.72  0.00
## Time:Diet4   0.00  0.00  0.00 10.72
\end{verbatim}
\end{kframe}
\end{knitrout}

The value of \cc{Time} is by default taken to be the average of that
variable. Hence the \lsmeans\ is the predicted weight for each diet at
that specific point of time. We can consider other points of time with
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{K1} \hlkwb{<-} \hlkwd{LE_matrix}\hlstd{(chick,} \hlkwc{effect}\hlstd{=}\hlstr{"Diet"}\hlstd{,} \hlkwc{at}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{Time}\hlstd{=}\hlnum{1}\hlstd{))}
\hlstd{> }\hlkwd{t}\hlstd{(K1)}
\end{alltt}
\begin{verbatim}
##             [,1] [,2] [,3] [,4]
## (Intercept)    1    1    1    1
## Time           1    1    1    1
## Diet2          0    1    0    0
## Diet3          0    0    1    0
## Diet4          0    0    0    1
## Time:Diet2     0    1    0    0
## Time:Diet3     0    0    1    0
## Time:Diet4     0    0    0    1
\end{verbatim}
\end{kframe}
\end{knitrout}

The \lsmeans\ for the intercepts is the predictions at
\cc{Time=0}. The \lsmeans\ for the slopes becomes
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{K0} \hlkwb{<-} \hlkwd{LE_matrix}\hlstd{(chick,} \hlkwc{effect}\hlstd{=}\hlstr{"Diet"}\hlstd{,} \hlkwc{at}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{Time}\hlstd{=}\hlnum{0}\hlstd{))}
\hlstd{> }\hlkwd{t}\hlstd{(K1} \hlopt{-} \hlstd{K0)}
\end{alltt}
\begin{verbatim}
##             [,1] [,2] [,3] [,4]
## (Intercept)    0    0    0    0
## Time           1    1    1    1
## Diet2          0    0    0    0
## Diet3          0    0    0    0
## Diet4          0    0    0    0
## Time:Diet2     0    1    0    0
## Time:Diet3     0    0    1    0
## Time:Diet4     0    0    0    1
\end{verbatim}
\begin{alltt}
\hlstd{> }\hlkwd{linest}\hlstd{(chick,} \hlkwc{L}\hlstd{=K1} \hlopt{-} \hlstd{K0)}
\end{alltt}
\begin{verbatim}
## Coefficients:
##      estimate std.error statistic     df p.value
## [1,]    6.339     0.610    10.383 49.855       0
## [2,]    8.609     0.838    10.273 48.282       0
## [3,]   11.423     0.838    13.631 48.282       0
## [4,]    9.556     0.839    11.386 48.565       0
\end{verbatim}
\end{kframe}
\end{knitrout}


% We may be interested in finding the population means
% at all levels of  $A$
% but only at $C=1$. This is obtained by using the \code{at} argument
% (in which case the average is only over the remaining factor $B$):

% <<>>=
% popMatrix(mm, effect='A', at=list(C='1'))
% @ %def

% Another way of
% creating the population means
% at  all levels of $(A,C)$ is therefore

% <<>>=
% popMatrix(mm, effect='A', at=list(C=c('1','2')))
% @ %def

% We may have several variables in the \code{at} argument:
% @
% <<>>=
% popMatrix(mm, effect='A', at=list(C=c('1','2'), B='1'))
% @ %def



We can create  our own function for comparing trends:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{LSmeans_trend} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{object}\hlstd{,} \hlkwc{effect}\hlstd{,} \hlkwc{trend}\hlstd{)\{}
\hlstd{  }    \hlstd{L} \hlkwb{<-} \hlkwd{LE_matrix}\hlstd{(object,} \hlkwc{effect}\hlstd{=effect,} \hlkwc{at}\hlstd{=}\hlkwd{as.list}\hlstd{(}\hlkwd{setNames}\hlstd{(}\hlnum{1}\hlstd{, trend)))} \hlopt{-}
\hlstd{  }        \hlkwd{LE_matrix}\hlstd{(object,} \hlkwc{effect}\hlstd{=effect,} \hlkwc{at}\hlstd{=}\hlkwd{as.list}\hlstd{(}\hlkwd{setNames}\hlstd{(}\hlnum{0}\hlstd{, trend)))}
\hlstd{  }    \hlkwd{linest}\hlstd{(object,} \hlkwc{L}\hlstd{=L)}
\hlstd{  }\hlstd{\}}
\hlstd{> }\hlkwd{LSmeans_trend}\hlstd{(chick,} \hlkwc{effect}\hlstd{=}\hlstr{"Diet"}\hlstd{,} \hlkwc{trend}\hlstd{=}\hlstr{"Time"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## Coefficients:
##      estimate std.error statistic     df p.value
## [1,]    6.339     0.610    10.383 49.855       0
## [2,]    8.609     0.838    10.273 48.282       0
## [3,]   11.423     0.838    13.631 48.282       0
## [4,]    9.556     0.839    11.386 48.565       0
\end{verbatim}
\end{kframe}
\end{knitrout}



\subsection{Ambiguous specification when using the \texttt{effect} and
  \texttt{at} arguments}

% There is room for an ambiguous specification if a variable appears in
% both the \code{effect} and the \code{at} argument, such as
% @
% <<>>=
% popMatrix(mm, effect=c('A','C'), at=list(C='1'))
% @ %def

% This ambiguity is due to the fact that the \verb+effect+ argument asks
% for the populations means at all levels of the variables but the
% \verb+at+ chooses only specific levels.

% This ambiguity is resolved as follows: Any variable in the \code{at}
% argument is removed from the \code{effect} argument such as the
% statement above is equivalent to
% @
% <<eval=T>>=
% ## popMatrix(mm, effect='A', at=list(C='1'))
% @ %def


\subsection{Using (transformed) covariates}
\label{sec:using-covariates}

Consider the following subset of the \code{CO2} dataset:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlkwd{data}\hlstd{(CO2)}
\hlstd{> }\hlstd{CO2} \hlkwb{<-} \hlkwd{transform}\hlstd{(CO2,} \hlkwc{Treat}\hlstd{=Treatment,} \hlkwc{Treatment}\hlstd{=}\hlkwa{NULL}\hlstd{)}
\hlstd{> }\hlkwd{levels}\hlstd{(CO2}\hlopt{$}\hlstd{Treat)} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"nchil"}\hlstd{,} \hlstr{"chil"}\hlstd{)}
\hlstd{> }\hlkwd{levels}\hlstd{(CO2}\hlopt{$}\hlstd{Type)} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"Que"}\hlstd{,} \hlstr{"Mis"}\hlstd{)}
\hlstd{> }\hlkwd{ftable}\hlstd{(}\hlkwd{xtabs}\hlstd{(} \hlopt{~} \hlstd{Plant} \hlopt{+} \hlstd{Type} \hlopt{+} \hlstd{Treat,} \hlkwc{data}\hlstd{=CO2),} \hlkwc{col.vars}\hlstd{=}\hlnum{2}\hlopt{:}\hlnum{3}\hlstd{)}
\end{alltt}
\begin{verbatim}
##       Type    Que        Mis     
##       Treat nchil chil nchil chil
## Plant                            
## Qn1             7    0     0    0
## Qn2             7    0     0    0
## Qn3             7    0     0    0
## Qc1             0    7     0    0
## Qc3             0    7     0    0
## Qc2             0    7     0    0
## Mn3             0    0     7    0
## Mn2             0    0     7    0
## Mn1             0    0     7    0
## Mc2             0    0     0    7
## Mc3             0    0     0    7
## Mc1             0    0     0    7
\end{verbatim}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlkwd{qplot}\hlstd{(}\hlkwc{x}\hlstd{=}\hlkwd{log}\hlstd{(conc),} \hlkwc{y}\hlstd{=uptake,} \hlkwc{data}\hlstd{=CO2,} \hlkwc{color}\hlstd{=Treat,} \hlkwc{facets}\hlstd{=}\hlopt{~}\hlstd{Type)}
\end{alltt}
\end{kframe}\begin{figure}
\includegraphics[width=\maxwidth]{figures/LSmeansco2-fig-1} \caption[CO2 data]{CO2 data}\label{fig:co2-fig}
\end{figure}

\end{knitrout}


Below, the covariate \code{conc} is fixed at the average value:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{co2.lm1} \hlkwb{<-} \hlkwd{lm}\hlstd{(uptake} \hlopt{~} \hlstd{conc} \hlopt{+} \hlstd{Type} \hlopt{+} \hlstd{Treat,} \hlkwc{data}\hlstd{=CO2)}
\hlstd{> }\hlkwd{LSmeans}\hlstd{(co2.lm1,} \hlkwc{effect}\hlstd{=}\hlstr{"Treat"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## Coefficients:
##      estimate std.error statistic     df p.value
## [1,]   30.643     0.956    32.066 80.000       0
## [2,]   23.783     0.956    24.888 80.000       0
\end{verbatim}
\end{kframe}
\end{knitrout}

If we use \code{log(conc)} instead we will get an error when
calculating LS--means:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{co2.lm} \hlkwb{<-} \hlkwd{lm}\hlstd{(uptake} \hlopt{~} \hlkwd{log}\hlstd{(conc)} \hlopt{+} \hlstd{Type} \hlopt{+} \hlstd{Treat,} \hlkwc{data}\hlstd{=CO2)}
\hlstd{> }\hlkwd{LSmeans}\hlstd{(co2.lm,} \hlkwc{effect}\hlstd{=}\hlstr{"Treat"}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

In this case one can do
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{co2.lm2} \hlkwb{<-} \hlkwd{lm}\hlstd{(uptake} \hlopt{~} \hlstd{log.conc} \hlopt{+} \hlstd{Type} \hlopt{+} \hlstd{Treat,}
\hlstd{  }             \hlkwc{data}\hlstd{=}\hlkwd{transform}\hlstd{(CO2,} \hlkwc{log.conc}\hlstd{=}\hlkwd{log}\hlstd{(conc)))}
\hlstd{> }\hlkwd{LSmeans}\hlstd{(co2.lm2,} \hlkwc{effect}\hlstd{=}\hlstr{"Treat"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## Coefficients:
##      estimate std.error statistic     df p.value
## [1,]   30.643     0.761    40.261 80.000       0
## [2,]   23.783     0.761    31.248 80.000       0
\end{verbatim}
\end{kframe}
\end{knitrout}

This also highlights what is computed: The average of the log of
\code{conc}; not the log of the average of \code{conc}.
In a similar spirit consider:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{co2.lm3} \hlkwb{<-} \hlkwd{lm}\hlstd{(uptake} \hlopt{~} \hlstd{conc} \hlopt{+} \hlkwd{I}\hlstd{(conc}\hlopt{^}\hlnum{2}\hlstd{)} \hlopt{+} \hlstd{Type} \hlopt{+} \hlstd{Treat,} \hlkwc{data}\hlstd{=CO2)}
\hlstd{> }\hlkwd{LSmeans}\hlstd{(co2.lm3,} \hlkwc{effect}\hlstd{=}\hlstr{"Treat"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## Coefficients:
##      estimate std.error statistic     df p.value
## [1,]   34.543     0.982    35.191 79.000       0
## [2,]   27.683     0.982    28.202 79.000       0
\end{verbatim}
\end{kframe}
\end{knitrout}

Above \verb'I(conc^2)' is the average of the squared values of
\code{conc}; not the  square of the average of
\code{conc}, cfr.\ the following.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{co2.lm4} \hlkwb{<-} \hlkwd{lm}\hlstd{(uptake} \hlopt{~} \hlstd{conc} \hlopt{+} \hlstd{conc2} \hlopt{+} \hlstd{Type} \hlopt{+} \hlstd{Treat,} \hlkwc{data}\hlstd{=}
\hlstd{  }              \hlkwd{transform}\hlstd{(CO2,} \hlkwc{conc2}\hlstd{=conc}\hlopt{^}\hlnum{2}\hlstd{))}
\hlstd{> }\hlkwd{LSmeans}\hlstd{(co2.lm4,} \hlkwc{effect}\hlstd{=}\hlstr{"Treat"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## Coefficients:
##      estimate std.error statistic     df p.value
## [1,]   30.643     0.776    39.465 79.000       0
## [2,]   23.783     0.776    30.630 79.000       0
\end{verbatim}
\end{kframe}
\end{knitrout}

If we want to evaluate the LS--means at \code{conc=10} then we can do:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlkwd{LSmeans}\hlstd{(co2.lm4,} \hlkwc{effect}\hlstd{=}\hlstr{"Treat"}\hlstd{,} \hlkwc{at}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{conc}\hlstd{=}\hlnum{10}\hlstd{,} \hlkwc{conc2}\hlstd{=}\hlnum{100}\hlstd{))}
\end{alltt}
\begin{verbatim}
## Coefficients:
##      estimate std.error statistic    df p.value
## [1,]    14.74      1.70      8.66 79.00       0
## [2,]     7.88      1.70      4.63 79.00       0
\end{verbatim}
\end{kframe}
\end{knitrout}


 

\section{Alternative models}
\label{sec:alternative-models}

\subsection{Generalized linear models}
\label{sec:gener-line-models}

We can calculate LS--means for e.g.\ a Poisson or a gamma model. Default is that
the calculation is calculated on the scale of the linear
predictor. However, if
we think of LS--means as a prediction on the linear scale one may
argue that it can also make sense to transform this prediction to
the response scale:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{tooth.gam} \hlkwb{<-} \hlkwd{glm}\hlstd{(len} \hlopt{~} \hlstd{dose} \hlopt{+} \hlstd{supp,} \hlkwc{family}\hlstd{=Gamma,} \hlkwc{data}\hlstd{=ToothGrowth)}
\hlstd{> }\hlkwd{LSmeans}\hlstd{(tooth.gam,} \hlkwc{effect}\hlstd{=}\hlstr{"dose"}\hlstd{,} \hlkwc{type}\hlstd{=}\hlstr{"link"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## Coefficients:
##      estimate std.error statistic p.value
## [1,]  0.09453   0.00579  16.33340       0
## [2,]  0.05111   0.00312  16.39673       0
## [3,]  0.03889   0.00238  16.36460       0
\end{verbatim}
\begin{alltt}
\hlstd{> }\hlkwd{LSmeans}\hlstd{(tooth.gam,} \hlkwc{effect}\hlstd{=}\hlstr{"dose"}\hlstd{,} \hlkwc{type}\hlstd{=}\hlstr{"response"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## Coefficients:
##      estimate std.error statistic p.value
## [1,]  0.09453   0.00579  16.33340       0
## [2,]  0.05111   0.00312  16.39673       0
## [3,]  0.03889   0.00238  16.36460       0
\end{verbatim}
\end{kframe}
\end{knitrout}


\subsection{Linear mixed effects model}
\label{sec:linear-mixed-effects}


For the sake of illustration we treat \verb|supp| as a random effect:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlkwd{library}\hlstd{(lme4)}
\hlstd{> }\hlstd{tooth.mm} \hlkwb{<-} \hlkwd{lmer}\hlstd{( len} \hlopt{~} \hlstd{dose}  \hlopt{+} \hlstd{(}\hlnum{1}\hlopt{|}\hlstd{supp),} \hlkwc{data}\hlstd{=ToothGrowth)}
\hlstd{> }\hlkwd{LSmeans}\hlstd{(tooth1,} \hlkwc{effect}\hlstd{=}\hlstr{"dose"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## Coefficients:
##      estimate std.error statistic     df p.value
## [1,]   10.605     0.856    12.391 56.000       0
## [2,]   19.735     0.856    23.058 56.000       0
## [3,]   26.100     0.856    30.495 56.000       0
\end{verbatim}
\begin{alltt}
\hlstd{> }\hlkwd{LSmeans}\hlstd{(tooth.mm,} \hlkwc{effect}\hlstd{=}\hlstr{"dose"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## Coefficients:
##      estimate std.error statistic    df p.value
## [1,]    10.61      1.98      5.36  1.31    0.08
## [2,]    19.74      1.98      9.98  1.31    0.03
## [3,]    26.10      1.98     13.20  1.31    0.02
\end{verbatim}
\end{kframe}
\end{knitrout}


Notice here that the estimates themselves identical to those of a
linear model (that is not generally the case, but it is so here
because data is balanced). In general the estimates are will be 
very similar but the standard errors are much larger under the mixed model.
This comes from that
there that \code{supp} is treated as a random effect.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlkwd{VarCorr}\hlstd{(tooth.mm)}
\end{alltt}
\begin{verbatim}
##  Groups   Name        Std.Dev.
##  supp     (Intercept) 2.52    
##  Residual             3.83
\end{verbatim}
\end{kframe}
\end{knitrout}

Notice that the degrees of freedom by default are adjusted using a
Kenward--Roger approximation (provided that \pkg{pbkrtest} is
installed). Unadjusted degrees of freedom are obtained by setting \verb|adjust.df=FALSE|. 


% Notice that the degrees of freedom by default are adjusted using a
% Kenward--Roger approximation (provided that \pkg{pbkrtest} is
% installed). Unadjusted degrees of freedom are obtained with
% <<>>=
% LSmeans(warp.mm, effect="tension", adjust.df=FALSE)
% @ %def

\subsection{Generalized estimating equations}
\label{sec:gener-estim-equat}

Lastly, for gee-type ``models'' we get
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlkwd{library}\hlstd{(geepack)}
\hlstd{> }\hlstd{tooth.gee} \hlkwb{<-} \hlkwd{geeglm}\hlstd{(len} \hlopt{~} \hlstd{dose,} \hlkwc{id}\hlstd{=supp,} \hlkwc{family}\hlstd{=Gamma,} \hlkwc{data}\hlstd{=ToothGrowth)}
\hlstd{> }\hlkwd{LSmeans}\hlstd{(tooth.gee,} \hlkwc{effect}\hlstd{=}\hlstr{"dose"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## Coefficients:
##      estimate std.error statistic p.value
## [1,] 9.43e-02  1.65e-02  5.71e+00       0
## [2,] 5.07e-02  5.38e-03  9.41e+00       0
## [3,] 3.83e-02  4.15e-05  9.23e+02       0
\end{verbatim}
\begin{alltt}
\hlstd{> }\hlkwd{LSmeans}\hlstd{(tooth.gee,} \hlkwc{effect}\hlstd{=}\hlstr{"dose"}\hlstd{,} \hlkwc{type}\hlstd{=}\hlstr{"response"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## Coefficients:
##      estimate std.error statistic p.value
## [1,] 9.43e-02  1.65e-02  5.71e+00       0
## [2,] 5.07e-02  5.38e-03  9.41e+00       0
## [3,] 3.83e-02  4.15e-05  9.23e+02       0
\end{verbatim}
\end{kframe}
\end{knitrout}


% Lastly, for gee-type ``models'' we get
% <<>>=
% library(geepack)
% warp.gee <- geeglm(breaks ~ tension, id=wool, family=poisson, data=warpbreaks)
% LSmeans(warp.gee, effect="tension")
% LSmeans(warp.gee, effect="tension", type="response")
% @ %def








\section{Miscellaneous}
\label{sec:miscellaneous}

% \subsection{Under the hood}
% \label{sec:under-hood}

% Under the hood, \cc{LSmeans()} generates a contrast matrix
% <<>>=
% K <- LE_matrix(warp.lm, effect="tension"); K
% @ %def
% and passes this matrix onto \linest:
% <<>>=
% linest( warp.lm, K=K )
% @ %def

\subsection{Example: Non--estimable linear functions}
\label{sec:exampl-non-estim}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlcom{## Make balanced dataset}
\hlstd{> }\hlstd{dat.bal} \hlkwb{<-} \hlkwd{expand.grid}\hlstd{(}\hlkwd{list}\hlstd{(}\hlkwc{AA}\hlstd{=}\hlkwd{factor}\hlstd{(}\hlnum{1}\hlopt{:}\hlnum{2}\hlstd{),} \hlkwc{BB}\hlstd{=}\hlkwd{factor}\hlstd{(}\hlnum{1}\hlopt{:}\hlnum{3}\hlstd{),} \hlkwc{CC}\hlstd{=}\hlkwd{factor}\hlstd{(}\hlnum{1}\hlopt{:}\hlnum{3}\hlstd{)))}
\hlstd{> }\hlstd{dat.bal}\hlopt{$}\hlstd{y} \hlkwb{<-} \hlkwd{rnorm}\hlstd{(}\hlkwd{nrow}\hlstd{(dat.bal))}
\hlstd{> }
\hlstd{> }\hlcom{## Make unbalanced dataset:  'BB' is nested within 'CC' so BB=1}
\hlstd{> }\hlcom{## is only found when CC=1 and BB=2,3 are found in each CC=2,3,4}
\hlstd{> }\hlstd{dat.nst} \hlkwb{<-} \hlstd{dat.bal}
\hlstd{> }\hlstd{dat.nst}\hlopt{$}\hlstd{CC} \hlkwb{<-}\hlkwd{factor}\hlstd{(}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,}\hlnum{1}\hlstd{,}\hlnum{2}\hlstd{,}\hlnum{2}\hlstd{,}\hlnum{2}\hlstd{,}\hlnum{2}\hlstd{,}\hlnum{1}\hlstd{,}\hlnum{1}\hlstd{,}\hlnum{3}\hlstd{,}\hlnum{3}\hlstd{,}\hlnum{3}\hlstd{,}\hlnum{3}\hlstd{,}\hlnum{1}\hlstd{,}\hlnum{1}\hlstd{,}\hlnum{4}\hlstd{,}\hlnum{4}\hlstd{,}\hlnum{4}\hlstd{,}\hlnum{4}\hlstd{))}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{dat.nst}
\end{alltt}
\begin{verbatim}
##    AA BB CC       y
## 1   1  1  1 -0.6565
## 2   2  1  1  0.5437
## 3   1  2  2 -0.4038
## 4   2  2  2  1.0040
## 5   1  3  2 -0.4669
## 6   2  3  2 -0.9808
## 7   1  1  1  0.3871
## 8   2  1  1  0.7652
## 9   1  2  3  1.6487
## 10  2  2  3  0.3072
## 11  1  3  3  1.0357
## 12  2  3  3 -0.6332
## 13  1  1  1 -0.5037
## 14  2  1  1  0.2796
## 15  1  2  4  0.1614
## 16  2  2  4  0.9293
## 17  1  3  4 -0.1734
## 18  2  3  4  1.0296
\end{verbatim}
\end{kframe}
\end{knitrout}

Consider this simulated dataset:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlkwd{head}\hlstd{(dat.nst,} \hlnum{4}\hlstd{)}
\end{alltt}
\begin{verbatim}
##   AA BB CC       y
## 1  1  1  1 -0.6565
## 2  2  1  1  0.5437
## 3  1  2  2 -0.4038
## 4  2  2  2  1.0040
\end{verbatim}
\begin{alltt}
\hlstd{> }\hlkwd{ftable}\hlstd{(}\hlkwd{xtabs}\hlstd{(} \hlopt{~} \hlstd{AA} \hlopt{+} \hlstd{BB} \hlopt{+} \hlstd{CC,} \hlkwc{data}\hlstd{=dat.nst),} \hlkwc{row.vars}\hlstd{=}\hlstr{"AA"}\hlstd{)}
\end{alltt}
\begin{verbatim}
##    BB 1       2       3      
##    CC 1 2 3 4 1 2 3 4 1 2 3 4
## AA                           
## 1     3 0 0 0 0 1 1 1 0 1 1 1
## 2     3 0 0 0 0 1 1 1 0 1 1 1
\end{verbatim}
\end{kframe}
\end{knitrout}

Data is highly "unbalanced":
Whenever \verb|BB=1| then \verb|CC| is always \verb|1|; whenever
\verb|BB| is not \verb|1| then \verb|CC| is never \verb|1|.
We have
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{mod.nst}  \hlkwb{<-} \hlkwd{lm}\hlstd{(y} \hlopt{~} \hlstd{AA} \hlopt{+} \hlstd{BB} \hlopt{:} \hlstd{CC,} \hlkwc{data}\hlstd{=dat.nst)}
\hlstd{> }\hlkwd{coef}\hlstd{(}\hlkwd{summary}\hlstd{(mod.nst))}
\end{alltt}
\begin{verbatim}
##             Estimate Std. Error t value Pr(>|t|)
## (Intercept)   0.3050     0.5698  0.5353   0.6041
## AA2           0.2462     0.3604  0.6832   0.5100
## BB1:CC1      -0.2922     0.6242 -0.4681   0.6497
## BB2:CC2      -0.1280     0.7645 -0.1675   0.8703
## BB3:CC2      -1.1519     0.7645 -1.5069   0.1628
## BB2:CC3       0.5499     0.7645  0.7193   0.4884
## BB3:CC3      -0.2269     0.7645 -0.2967   0.7727
## BB2:CC4       0.1173     0.7645  0.1534   0.8811
\end{verbatim}
\end{kframe}
\end{knitrout}


In this case some of the \lsmeans\ values are not estimable; for example:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{lsm.BC} \hlkwb{<-} \hlkwd{LSmeans}\hlstd{(mod.nst,} \hlkwc{effect}\hlstd{=}\hlkwd{c}\hlstd{(}\hlstr{"BB"}\hlstd{,} \hlstr{"CC"}\hlstd{))}
\hlstd{> }\hlstd{lsm.BC}
\end{alltt}
\begin{verbatim}
## Coefficients:
##       estimate std.error statistic     df p.value
##  [1,]    0.136     0.312     0.435 10.000    0.67
##  [2,]       NA        NA        NA     NA      NA
##  [3,]       NA        NA        NA     NA      NA
##  [4,]       NA        NA        NA     NA      NA
##  [5,]    0.300     0.541     0.555 10.000    0.59
##  [6,]   -0.724     0.541    -1.339 10.000    0.21
##  [7,]       NA        NA        NA     NA      NA
##  [8,]    0.978     0.541     1.809 10.000    0.10
##  [9,]    0.201     0.541     0.372 10.000    0.72
## [10,]       NA        NA        NA     NA      NA
## [11,]    0.545     0.541     1.009 10.000    0.34
## [12,]    0.428     0.541     0.792 10.000    0.45
\end{verbatim}
\begin{alltt}
\hlstd{> }\hlstd{lsm.BC2} \hlkwb{<-} \hlkwd{LSmeans}\hlstd{(mod.nst,} \hlkwc{effect}\hlstd{=}\hlstr{"BB"}\hlstd{,} \hlkwc{at}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{CC}\hlstd{=}\hlnum{2}\hlstd{))}
\hlstd{> }\hlstd{lsm.BC2}
\end{alltt}
\begin{verbatim}
## Coefficients:
##      estimate std.error statistic     df p.value
## [1,]       NA        NA        NA     NA      NA
## [2,]    0.300     0.541     0.555 10.000    0.59
## [3,]   -0.724     0.541    -1.339 10.000    0.21
\end{verbatim}
\end{kframe}
\end{knitrout}

We describe the situation in 
Section~\ref{sec:handl-non-estim} where we focus on \verb|lsm.BC2|.

\subsection{Handling non--estimability}
\label{sec:handl-non-estim}

The model matrix for the model in Section~\ref{sec:exampl-non-estim}
does not have full column rank and therefore not all values are
calculated by \cc{LSmeans()}.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{X} \hlkwb{<-} \hlkwd{model.matrix}\hlstd{( mod.nst )}
\hlstd{> }\hlstd{Matrix}\hlopt{::}\hlkwd{rankMatrix}\hlstd{(X)}
\end{alltt}
\begin{verbatim}
## [1] 8
## attr(,"method")
## [1] "tolNorm2"
## attr(,"useGrad")
## [1] FALSE
## attr(,"tol")
## [1] 3.997e-15
\end{verbatim}
\begin{alltt}
\hlstd{> }\hlkwd{dim}\hlstd{(X)}
\end{alltt}
\begin{verbatim}
## [1] 18 14
\end{verbatim}
\begin{alltt}
\hlstd{> }\hlkwd{as}\hlstd{(X,} \hlstr{"Matrix"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## 18 x 14 sparse Matrix of class "dgCMatrix"
\end{verbatim}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \ \ \ [[ suppressing 14 column names '(Intercept)', 'AA2', 'BB1:CC1' ... ]]}}\begin{verbatim}
##                               
## 1  1 . 1 . . . . . . . . . . .
## 2  1 1 1 . . . . . . . . . . .
## 3  1 . . . . . 1 . . . . . . .
## 4  1 1 . . . . 1 . . . . . . .
## 5  1 . . . . . . 1 . . . . . .
## 6  1 1 . . . . . 1 . . . . . .
## 7  1 . 1 . . . . . . . . . . .
## 8  1 1 1 . . . . . . . . . . .
## 9  1 . . . . . . . . 1 . . . .
## 10 1 1 . . . . . . . 1 . . . .
## 11 1 . . . . . . . . . 1 . . .
## 12 1 1 . . . . . . . . 1 . . .
## 13 1 . 1 . . . . . . . . . . .
## 14 1 1 1 . . . . . . . . . . .
## 15 1 . . . . . . . . . . . 1 .
## 16 1 1 . . . . . . . . . . 1 .
## 17 1 . . . . . . . . . . . . 1
## 18 1 1 . . . . . . . . . . . 1
\end{verbatim}
\end{kframe}
\end{knitrout}

We consider a  model, i.e.\
an $n$ dimensional random vector $y=(y_i)$ for which
$\EE(y)=\mu=X\beta$ and $\cov(y)=V$ where $X$ does not have
full column rank
We are
interested in linear functions of $\beta$, say
\begin{displaymath}
  c=l\transp\beta= \sum_j l_j \beta_j .
\end{displaymath}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{L} \hlkwb{<-} \hlkwd{LE_matrix}\hlstd{(mod.nst,} \hlkwc{effect}\hlstd{=}\hlstr{"BB"}\hlstd{,} \hlkwc{at}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{CC}\hlstd{=}\hlnum{2}\hlstd{))}
\hlstd{> }\hlkwd{t}\hlstd{(L)}
\end{alltt}
\begin{verbatim}
##             [,1] [,2] [,3]
## (Intercept)  1.0  1.0  1.0
## AA2          0.5  0.5  0.5
## BB1:CC1      0.0  0.0  0.0
## BB2:CC1      0.0  0.0  0.0
## BB3:CC1      0.0  0.0  0.0
## BB1:CC2      1.0  0.0  0.0
## BB2:CC2      0.0  1.0  0.0
## BB3:CC2      0.0  0.0  1.0
## BB1:CC3      0.0  0.0  0.0
## BB2:CC3      0.0  0.0  0.0
## BB3:CC3      0.0  0.0  0.0
## BB1:CC4      0.0  0.0  0.0
## BB2:CC4      0.0  0.0  0.0
## BB3:CC4      0.0  0.0  0.0
\end{verbatim}
\begin{alltt}
\hlstd{> }\hlkwd{linest}\hlstd{(mod.nst,} \hlkwc{L}\hlstd{=L)}
\end{alltt}
\begin{verbatim}
## Coefficients:
##      estimate std.error statistic     df p.value
## [1,]       NA        NA        NA     NA      NA
## [2,]    0.300     0.541     0.555 10.000    0.59
## [3,]   -0.724     0.541    -1.339 10.000    0.21
\end{verbatim}
\end{kframe}
\end{knitrout}

A least squares estimate of $\beta$ is
\begin{displaymath}
  \hat \beta = G X\transp y
\end{displaymath}

where $G$ is a generalized inverse of $X\transp  X$. Since the
generalized inverse is not unique then neither is the estimate
$\hat\beta$. Hence $\hat c = l\transp\hat \beta$ is in general not
unique.

One least squares estimate of $\beta$ and one corresponding linear estimate $L\hat\beta$ is: 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{XtXinv} \hlkwb{<-} \hlstd{MASS}\hlopt{::}\hlkwd{ginv}\hlstd{(}\hlkwd{t}\hlstd{(X)}\hlopt{%*%}\hlstd{X)}
\hlstd{> }\hlstd{bhat} \hlkwb{<-} \hlkwd{as.numeric}\hlstd{(XtXinv} \hlopt{%*%} \hlkwd{t}\hlstd{(X)} \hlopt{%*%} \hlstd{dat.nst}\hlopt{$}\hlstd{y)}
\hlstd{> }\hlkwd{zapsmall}\hlstd{(bhat)}
\end{alltt}
\begin{verbatim}
##  [1]  0.1254  0.2462 -0.1126  0.0000  0.0000  0.0000  0.0516 -0.9723  0.0000  0.7295
## [11] -0.0472  0.0000  0.2969  0.1796
\end{verbatim}
\begin{alltt}
\hlstd{> }\hlstd{L} \hlopt{%*%} \hlstd{bhat}
\end{alltt}
\begin{verbatim}
##         [,1]
## [1,]  0.2485
## [2,]  0.3001
## [3,] -0.7238
\end{verbatim}
\end{kframe}
\end{knitrout}

For some values of $l$ (i.e.\ for some rows of $L$)
the estimate $\hat c=l\transp \beta$ is
unique (i.e.\ it does not depend on the choice of generalized
inverse). 
Such linear functions are said to be estimable and can be
described as follows:

All we specify with $\mu=X\beta$
is that $\mu$ is a vector in the column space $C(X)$ of $X$.
We can only learn about $\beta$ through $X\beta$ so the only thing we can
say something about is linear combinations $\rho\transp X\beta$. Hence
we can only say something about $l\transp\beta$ if there exists
$\rho$ such that 
$$
l\transp\beta=\rho\transp X \beta,
$$ 
i.e., if
$l=X\transp\rho$ for some $\rho$, which is if $l$ is in the column space
$C(X\transp)$ of $X\transp$. This is the same as saying that $l$ must be 
perpendicular to
all vectors in the null space $N(X)$ of $X$. To check
this, we find a basis $B$ for $N(X)$. This can be done in many ways,
for example via a singular value decomposition of $X$, i.e.\
\begin{displaymath}
  X = U D V\transp
\end{displaymath}
A basis for $N(X)$ is given by those columns of $V$ that corresponds
to zeros on the diagonal of $D$.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{S} \hlkwb{<-} \hlkwd{svd}\hlstd{(X)}
\hlstd{> }\hlstd{B} \hlkwb{<-} \hlstd{S}\hlopt{$}\hlstd{v[, S}\hlopt{$}\hlstd{d} \hlopt{<} \hlnum{1e-10}\hlstd{,} \hlkwc{drop}\hlstd{=}\hlnum{FALSE} \hlstd{];}
\hlstd{> }\hlkwd{head}\hlstd{(B)} \hlcom{## Basis for N(X)}
\end{alltt}
\begin{verbatim}
##           [,1]       [,2]       [,3]       [,4]       [,5] [,6]
## [1,]  0.339176 -5.635e-04  9.968e-02 -4.350e-03 -2.274e-03    0
## [2,]  0.000000  1.193e-17 -1.110e-16  1.735e-18  4.337e-19    0
## [3,] -0.339176  5.635e-04 -9.968e-02  4.350e-03  2.274e-03    0
## [4,] -0.272743 -2.494e-01  9.244e-01 -3.167e-03 -9.422e-02    0
## [5,] -0.072691  9.176e-01  2.509e-01 -1.669e-01  2.487e-01    0
## [6,] -0.001889 -9.509e-02  5.169e-02  6.615e-01  7.421e-01    0
\end{verbatim}
\end{kframe}
\end{knitrout}

From 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlkwd{rowSums}\hlstd{(L} \hlopt{%*%} \hlstd{B)}
\end{alltt}
\begin{verbatim}
## [1]  1.790e+00  1.632e-15 -4.113e-15
\end{verbatim}
\end{kframe}
\end{knitrout}
we conclude that the first row of $L$ is not perpendicular to all vectors in thenull space $N(X)$ whereas the two last rows of $L$ are. Hence these two linear estimates are estimable; their value does not depend on the choice of generalized inverse:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{lsm.BC2}
\end{alltt}
\begin{verbatim}
## Coefficients:
##      estimate std.error statistic     df p.value
## [1,]       NA        NA        NA     NA      NA
## [2,]    0.300     0.541     0.555 10.000    0.59
## [3,]   -0.724     0.541    -1.339 10.000    0.21
\end{verbatim}
\end{kframe}
\end{knitrout}


\subsection{Pairwise comparisons}
\label{sec:pairwise-comparisons}

We will just mention that for certain other linear estimates, the
matrix $L$ can be generated automatically using \cc{glht()} from the
\pkg{multcomp} package. For example, pairwise comparisons of all
levels of \code{dose} can be obtained with

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlkwd{library}\hlstd{(}\hlstr{"multcomp"}\hlstd{)}
\hlstd{> }\hlstd{g1} \hlkwb{<-} \hlkwd{glht}\hlstd{(tooth1,} \hlkwd{mcp}\hlstd{(}\hlkwc{dose}\hlstd{=}\hlstr{"Tukey"}\hlstd{))}
\hlstd{> }\hlkwd{summary}\hlstd{( g1 )}
\end{alltt}
\begin{verbatim}
## 
## 	 Simultaneous Tests for General Linear Hypotheses
## 
## Multiple Comparisons of Means: Tukey Contrasts
## 
## 
## Fit: lm(formula = len ~ dose + supp, data = ToothGrowth)
## 
## Linear Hypotheses:
##              Estimate Std. Error t value Pr(>|t|)    
## 1 - 0.5 == 0     9.13       1.21    7.54  < 1e-06 ***
## 2 - 0.5 == 0    15.49       1.21   12.80  < 1e-06 ***
## 2 - 1 == 0       6.36       1.21    5.26  5.5e-06 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## (Adjusted p values reported -- single-step method)
\end{verbatim}
\end{kframe}
\end{knitrout}

The L matrix is 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{L} \hlkwb{<-} \hlstd{g1}\hlopt{$}\hlstd{linfct}
\hlstd{> }\hlstd{L}
\end{alltt}
\begin{verbatim}
##         (Intercept) dose1 dose2 suppVC
## 1 - 0.5           0     1     0      0
## 2 - 0.5           0     0     1      0
## 2 - 1             0    -1     1      0
## attr(,"type")
## [1] "Tukey"
\end{verbatim}
\end{kframe}
\end{knitrout}
and this matrix can also be supplied to \verb|glht|
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlkwd{glht}\hlstd{(tooth1,} \hlkwc{linfct}\hlstd{=L)}
\end{alltt}
\end{kframe}
\end{knitrout}



% <<>>=
% library("multcomp")
% g1 <- glht(warp.lm, mcp(tension="Tukey"))
% summary( g1 )
% @ %def

% The $K$ matrix generated in this case is:
% <<>>=
% K1 <- g1$linfct; K1
% @ %def








%%% --------------------------------------------------------------------
%%% from half.Rnw
%%% --------------------------------------------------------------------

\section{LSmeans (population means, marginal means)}
\label{sec:xxx}

\subsection{A simulated dataset}
\label{sec:simulated-dataset}

In the following sections we consider these data:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlkwd{library}\hlstd{(doBy)}
\hlstd{> }\hlstd{dd} \hlkwb{<-} \hlkwd{expand.grid}\hlstd{(}\hlkwc{A}\hlstd{=}\hlkwd{factor}\hlstd{(}\hlnum{1}\hlopt{:}\hlnum{3}\hlstd{),}\hlkwc{B}\hlstd{=}\hlkwd{factor}\hlstd{(}\hlnum{1}\hlopt{:}\hlnum{3}\hlstd{),}\hlkwc{C}\hlstd{=}\hlkwd{factor}\hlstd{(}\hlnum{1}\hlopt{:}\hlnum{2}\hlstd{))}
\hlstd{> }\hlstd{dd}\hlopt{$}\hlstd{y} \hlkwb{<-} \hlkwd{rnorm}\hlstd{(}\hlkwd{nrow}\hlstd{(dd))}
\hlstd{> }\hlstd{dd}\hlopt{$}\hlstd{x} \hlkwb{<-} \hlkwd{rnorm}\hlstd{(}\hlkwd{nrow}\hlstd{(dd))}\hlopt{^}\hlnum{2}
\hlstd{> }\hlstd{dd}\hlopt{$}\hlstd{z} \hlkwb{<-} \hlkwd{rnorm}\hlstd{(}\hlkwd{nrow}\hlstd{(dd))}
\hlstd{> }\hlkwd{head}\hlstd{(dd,}\hlnum{10}\hlstd{)}
\end{alltt}
\begin{verbatim}
##    A B C        y       x       z
## 1  1 1 1 -1.49546 0.78288  1.0808
## 2  2 1 1 -0.37917 1.42919  0.3328
## 3  3 1 1  0.30257 0.03586  1.4263
## 4  1 2 1  0.23700 0.20167  0.5502
## 5  2 2 1 -0.14892 0.89591  0.1955
## 6  3 2 1  1.13943 0.04060  1.0272
## 7  1 3 1 -1.20054 0.09584  0.1798
## 8  2 3 1  0.65590 0.19273 -0.9473
## 9  3 3 1  0.04295 1.09377  0.2569
## 10 1 1 2 -1.86757 0.13544 -1.3944
\end{verbatim}
\end{kframe}
\end{knitrout}

Consider the additive model
\begin{equation}
  \label{eq:1}
  y_i = \beta_0 + \beta^1_{A(i)}+\beta^2_{B(i)} + \beta^3_{C(i)} + e_i
\end{equation}
where $e_i \sim N(0,\sigma^2)$. We fit this model:


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{mm} \hlkwb{<-} \hlkwd{lm}\hlstd{(y}\hlopt{~}\hlstd{A}\hlopt{+}\hlstd{B}\hlopt{+}\hlstd{C,} \hlkwc{data}\hlstd{=dd)}
\hlstd{> }\hlkwd{coef}\hlstd{(mm)}
\end{alltt}
\begin{verbatim}
## (Intercept)          A2          A3          B2          B3          C2 
##     -1.3719      0.5688      1.2390      1.3907      0.6351      0.2500
\end{verbatim}
\end{kframe}
\end{knitrout}

Notice that the parameters corresponding to the factor levels
\code{A1}, \code{B1} and \code{C2} are set to zero to ensure
identifiability of the remaining parameters.

\subsection{What are these quantities}
\label{sec:what-are-these}

LSmeans, population means and marginal means are used synonymously in
the literature. 
These quantities are a special kind of contrasts as defined in
Section~\ref{sec:line-funct-param}.
LSmeans seems to be the most widely used term, so we
shall adopt this terms here too.

The model (\ref{eq:1})
is a model for the conditional mean $\EE(y|A,B,C)$.  Sometimes one is
interested in quantities like $\EE(y|A)$. This quantity can not
formally be found unless $B$ and $C$ are random variables such that we
may find $\EE(y|A)$ by integration.
However, suppose that $A$ is a treatment of main interest, $B$ is a
blocking factor and $C$ represents days on which the experiment was
carried out. Then it is tempting to average $\EE(y|A,B,C)$ over $B$
and $C$ (average over block and day) and think of this average as
$\EE(y|A)$.

% \subsection{A brute--force calculation}
% \label{sec:xxx}

The population mean for $A=1$ is
\begin{equation}
  \label{eq:2}
  \beta^0 + \beta^1_{A1} + \frac{1}{3} (\beta^2_{B1}+\beta^2_{B2}+\beta^2_{B3})
  + \frac{1}{2}(\beta^3_{C1}+\beta^3_{C2})
\end{equation}

Recall that the
parameters corresponding to the factor levels
\code{A1}, \code{B1} and \code{C2} are set to zero to ensure
identifiability of the remaining parameters. Therefore we may also
write the population mean for $A=1$ as
\begin{equation}
  \label{eq:3}
  \beta^0 + \frac{1}{3} (\beta^2_{B2}+\beta^2_{B3})
  + \frac{1}{2}(\beta^3_{C2})
\end{equation}


This quantity can be estimated as:

@
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{w} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{1}\hlopt{/}\hlnum{3}\hlstd{,} \hlnum{1}\hlopt{/}\hlnum{3}\hlstd{,} \hlnum{1}\hlopt{/}\hlnum{2}\hlstd{)}
\hlstd{> }\hlkwd{coef}\hlstd{(mm)}\hlopt{*}\hlstd{w}
\end{alltt}
\begin{verbatim}
## (Intercept)          A2          A3          B2          B3          C2 
##     -1.3719      0.0000      0.0000      0.4636      0.2117      0.1250
\end{verbatim}
\begin{alltt}
\hlstd{> }\hlkwd{sum}\hlstd{(}\hlkwd{coef}\hlstd{(mm)}\hlopt{*}\hlstd{w)}
\end{alltt}
\begin{verbatim}
## [1] -0.5716
\end{verbatim}
\end{kframe}
\end{knitrout}


We may find the population mean for all three levels of $A$ as

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{W} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{1}\hlopt{/}\hlnum{3}\hlstd{,} \hlnum{1}\hlopt{/}\hlnum{3}\hlstd{,} \hlnum{1}\hlopt{/}\hlnum{2}\hlstd{,}
\hlstd{  }              \hlnum{1}\hlstd{,} \hlnum{1}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{1}\hlopt{/}\hlnum{3}\hlstd{,} \hlnum{1}\hlopt{/}\hlnum{3}\hlstd{,} \hlnum{1}\hlopt{/}\hlnum{2}\hlstd{,}
\hlstd{  }              \hlnum{1}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{1}\hlstd{,} \hlnum{1}\hlopt{/}\hlnum{3}\hlstd{,} \hlnum{1}\hlopt{/}\hlnum{3}\hlstd{,} \hlnum{1}\hlopt{/}\hlnum{2}\hlstd{),}\hlkwc{nr}\hlstd{=}\hlnum{3}\hlstd{,} \hlkwc{byrow}\hlstd{=}\hlnum{TRUE}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

Notice that the matrix W is based on that the first level of $A$ is
set as the reference level. If the reference level is changed then so
must $W$ be.

% \subsection{Using \esticon}
% \label{sec:xxx}

Given that one has specified $W$, we can use the \esticon\ function in the
\code{doBy} as illustrated below:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlkwd{esticon}\hlstd{(mm, W)}
\end{alltt}
\begin{verbatim}
##      estimate std.error statistic  p.value    beta0 df
## [1,] -0.57163   0.24404  -2.34237  0.03723  0.00000 12
## [2,] -0.00284   0.24404  -0.01163  0.99091  0.00000 12
## [3,]  0.66741   0.24404   2.73481  0.01810  0.00000 12
\end{verbatim}
\end{kframe}
\end{knitrout}

\def\popmatrix{POPMATRIX}
\def\popmeans{POPMEANS}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{popMatrix} \hlkwb{<-} \hlstd{LE_matrix}
\hlstd{> }\hlstd{popMeans} \hlkwb{<-} \hlstd{LSmeans}
\end{alltt}
\end{kframe}
\end{knitrout}

\subsection{Using \popmatrix\  and \popmeans}
\label{sec:xxx}

Writing the matrix $W$ is somewhat tedious and hence error prone. In
addition, there is a potential risk of getting the wrong answer if the
the reference level of a factor has been changed.
The \popmatrix\ function provides an automated way of generating
such matrices.
The above \verb+W+ matrix is  constructed by

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{pma} \hlkwb{<-} \hlkwd{popMatrix}\hlstd{(mm,}\hlkwc{effect}\hlstd{=}\hlstr{'A'}\hlstd{)}
\hlstd{> }\hlkwd{summary}\hlstd{(pma)}
\end{alltt}
\begin{verbatim}
##      (Intercept) A2 A3     B2     B3  C2
## [1,]           1  0  0 0.3333 0.3333 0.5
## [2,]           1  1  0 0.3333 0.3333 0.5
## [3,]           1  0  1 0.3333 0.3333 0.5
## at: 
## NULL
## grid: 
##   A
## 1 1
## 2 2
## 3 3
\end{verbatim}
\end{kframe}
\end{knitrout}

The \verb+effect+ argument requires  to calculate the population means
for each level of
$A$ aggregating across the levels of the other variables in the data.

The \popmeans\ function is simply a wrapper around first a call
to \popmatrix\ followed by a call to (by default) \esticon:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{pme} \hlkwb{<-} \hlkwd{popMeans}\hlstd{(mm,} \hlkwc{effect}\hlstd{=}\hlstr{'A'}\hlstd{)}
\hlstd{> }\hlstd{pme}
\end{alltt}
\begin{verbatim}
## Coefficients:
##      estimate std.error statistic       df p.value
## [1,] -0.57163   0.24404  -2.34237 12.00000    0.04
## [2,] -0.00284   0.24404  -0.01163 12.00000    0.99
## [3,]  0.66741   0.24404   2.73481 12.00000    0.02
\end{verbatim}
\end{kframe}
\end{knitrout}

More details about how the matrix was constructed is provided by the
\code{summary()} function:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlkwd{summary}\hlstd{(pme)}
\end{alltt}
\begin{verbatim}
## Coefficients:
##      estimate std.error statistic       df p.value
## [1,] -0.57163   0.24404  -2.34237 12.00000    0.04
## [2,] -0.00284   0.24404  -0.01163 12.00000    0.99
## [3,]  0.66741   0.24404   2.73481 12.00000    0.02
## 
## Grid:
##   A
## 1 1
## 2 2
## 3 3
## 
## L:
##      (Intercept) A2 A3     B2     B3  C2
## [1,]           1  0  0 0.3333 0.3333 0.5
## [2,]           1  1  0 0.3333 0.3333 0.5
## [3,]           1  0  1 0.3333 0.3333 0.5
\end{verbatim}
\end{kframe}
\end{knitrout}


As an additional example we may do:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlkwd{popMatrix}\hlstd{(mm,}\hlkwc{effect}\hlstd{=}\hlkwd{c}\hlstd{(}\hlstr{'A'}\hlstd{,}\hlstr{'C'}\hlstd{))}
\end{alltt}
\begin{verbatim}
##      (Intercept) A2 A3     B2     B3 C2
## [1,]           1  0  0 0.3333 0.3333  0
## [2,]           1  1  0 0.3333 0.3333  0
## [3,]           1  0  1 0.3333 0.3333  0
## [4,]           1  0  0 0.3333 0.3333  1
## [5,]           1  1  0 0.3333 0.3333  1
## [6,]           1  0  1 0.3333 0.3333  1
\end{verbatim}
\end{kframe}
\end{knitrout}
This gives the matrix for calculating the estimate for each
combination of \code{A} and \code{C} when averaging over \code{B}.

Omitting \code{effect} as in

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlkwd{popMatrix}\hlstd{(mm)}
\end{alltt}
\begin{verbatim}
##      (Intercept)     A2     A3     B2     B3  C2
## [1,]           1 0.3333 0.3333 0.3333 0.3333 0.5
\end{verbatim}
\begin{alltt}
\hlstd{> }\hlkwd{popMeans}\hlstd{(mm)}
\end{alltt}
\begin{verbatim}
## Coefficients:
##      estimate std.error statistic     df p.value
## [1,]    0.031     0.141     0.220 12.000    0.83
\end{verbatim}
\end{kframe}
\end{knitrout}
gives the ``total average''.


% \subsection{Using the \code{at} argument}

% We may be interested in finding the population means
% at all levels of  $A$
% but only at $C=1$. This is obtained by using the \code{at} argument
% (in which case the average is only over the remaining factor $B$):

% <<>>=
% popMatrix(mm, effect='A', at=list(C='1'))
% @ %def

% Another way of
% creating the population means
% at  all levels of $(A,C)$ is therefore

% <<>>=
% popMatrix(mm, effect='A', at=list(C=c('1','2')))
% @ %def

% We may have several variables in the \code{at} argument:
% @
% <<>>=
% popMatrix(mm, effect='A', at=list(C=c('1','2'), B='1'))
% @ %def

% \subsection{Ambiguous specification when using the \texttt{effect} and
%   \texttt{at} arguments}

% There is room for an ambiguous specification if a variable appears in
% both the \code{effect} and the \code{at} argument, such as
% @
% <<>>=
% popMatrix(mm, effect=c('A','C'), at=list(C='1'))
% @ %def

% This ambiguity is due to the fact that the \verb+effect+ argument asks
% for the populations means at all levels of the variables but the
% \verb+at+ chooses only specific levels.

% This ambiguity is resolved as follows: Any variable in the \code{at}
% argument is removed from the \code{effect} argument such as the
% statement above is equivalent to
% @
% <<eval=T>>=
% ## popMatrix(mm, effect='A', at=list(C='1'))
% @ %def

% \subsection{Using covariates}

% Next consider the model where a covariate is included:
% @
% <<>>=
% mm2 <- lm(y~A+B+C+C:x, data=dd)
% coef(mm2)
% @ %def

% In this case we get
% <<>>=
% popMatrix(mm2,effect='A', at=list(C='1'))
% @ %def

% Above, $x$ has been replaced by its average and that is the general
% rule for models including covariates. However we may use the \code{at}
% argument to ask for calculation of the population mean at some
% user-specified value of $x$, say 12:
% <<>>=
% popMatrix(mm2,effect='A', at=list(C='1',x=12))
% @ %def

 
% <<>>=
% mm22 <- lm(y~A+B+C+C:x+I(x^2), data=dd)
% coef(mm22)
% @ %def 

 
% <<>>=
% popMatrix(mm22,effect='A', at=list(C='1'))
% @ %def 


 
% <<>>=
% dd <- transform(dd, x.sq=x^2)
% mm23 <- lm(y~A+B+C+C:x+x.sq, data=dd)
% coef(mm23)
% popMatrix(mm23,effect='A', at=list(C='1'))
% @ %def 


% \subsection{Using transformed covariates}

% Next consider the model where a  transformation of a covariate is included:
% @
% <<>>=
% mm3 <- lm(y~A+B+C+C:I(log(x)), data=dd)
% coef(summary(mm3))
% @ %def

% In this case we can not use \popmatrix\ (and hence
% \popmeans) directly.  Instead we first have to
% generate a new variable, say \verb+log.x+, with
% \verb+log.x+$=\log(x)$, in the data and then proceed as

% <<>>=
% dd <- transform(dd, log.x = log(x))
% mm32 <- lm(y~A+B+C+C:log.x, data=dd)
% popMatrix(mm32, effect='A', at=list(C='1'))
% @ %def

% \subsection{The \code{engine} argument of \popmeans}

% The \popmatrix is a function to generate a linear tranformation matrix of the model
% parameters with emphasis on  constructing such matrices for population
% means.
% \popmeans\ invokes by default the \esticon\ function on this
% linear transformation matrix for calculating parameter estimates and
% confidecne intervals.
% A similar function to \esticon\ is the \verb+glht+ function of the \verb+multcomp+
%  package.

%  The \code{glht()} function
%  can be chosen via the \verb+engine+ argument of \popmeans:

% <<>>=
%  library(multcomp)
% g<-popMeans(mm,effect='A', at=list(C='1'),engine="glht")
% g
% @ %def

% This allows to apply the methods available on the \verb+glht+ object like

% <<>>=
% summary(g,test=univariate())
% confint(g,calpha=univariate_calpha())
% @
% which yield the same results as the \esticon\ function.

% By default the functions will adjust the tests  and confidence intervals for multiplicity
% <<>>=
% summary(g)
% confint(g)
% @




















%%\bibliography{doBy}


\end{document}





% A special type of linear estimates is the so called least--squares
% means (or LS--means). Other names for these estimates include
% population means and marginal means. Consider an imaginary field
% experiment analyzed with model of the form
% <<eval=F>>=
% lm( y ~ treat + block + year)
% @ %def
% where \cc{treat} is a treatment factor, \cc{block} is a blocking
% factor and \cc{year} is the year (a factor) where the experiment is
% repeated over several years. This model specifies the conditional mean
% $\EE(Y|\cc{treat}, \cc{block},\cc{year})$. One may be interested in
% predictions of the
% form $\EE(Y|\cc{treat})$. This quantity can not formally be found from the
% model. However, it is tempting to average the fitted values of
% $\EE(Y|\cc{treat}, \cc{block},\cc{year})$ across the levels of
% \cc{block} and \cc{year} and think of this average as
% $\EE(Y|\cc{treat})$. This average is precisely what is called the
% LS--means. If the experiment is balanced then this average is
% identical to the average of the observations when stratified according
% to \cc{treat}.



% An alternative is to think of \cc{block} and \cc{year} as random
% effects, for example:
% <<eval=F>>=
% library(lme4)
% lmer( y ~ treat + (1|block) + (1|year))
% @ %def

% In this case one would directly obtain $\EE(Y|\cc{treat})$ from the
% model. However, there are at least two reasons why one may be hesitant
% to consider such a random effects model.
% \begin{itemize}
% \item Suppose there are three
% blocks and the experiment is repeated over three consecutive
% years. This means that the random effects are likely to be estimated
% with a large uncertainty (the estimates will have only two degrees of
% freedom).
% \item Furthermore, treating \cc{block} and \cc{year} as random
% effects means they should in principle come from a large population of
% possible blocks and years. This may or may not be reasonable for the
% blocks, but it is certainly a dubious assumption for the years.
% \end{itemize}






% \section{LS--means for linear models}
% \label{sec:linear-model}


% \subsection{LS--means -- a first example}
% \label{sec:ls-means-population}

% <<echo=F>>=
% simdat<-structure(list(treat = structure(c(1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L
% ), .Label = c("t1", "t2"), class = "factor"), year = structure(c(1L,
% 1L, 1L, 1L, 2L, 2L, 2L, 2L), .Label = c("1", "2"), class = "factor"),
%     y = c(0.5, 1, 1.5, 3, 3, 4.5, 5, 5.5)), .Names = c("treat", "year",
% "y"), row.names = c(NA, -8L), class = "data.frame")
% @ %def

% Consider these simulated data
% <<>>=
% simdat
% @ %def
% shown in the figure below.
% <<simdat-fig,include=F,fig.height=3, fig.width=6>>=
% library(ggplot2)
% qplot(treat, y, data=simdat, color=year, size=I(3))
% @ %def
% \includegraphics[height=6cm,width=12cm]{figures/LSmeans-simdat-fig}

% The LS--means under an additive model for the factor \cc{treat} is
% <<>>=
% msim <- lm(y ~ treat + year, data=simdat)
% LSmeans( msim, effect="treat")
% @ %def
% whereas the population means are
% <<>>=
% summaryBy(y~treat, data=simdat, FUN=mean)
% @ %def
% Had data been balanced (same number of observations for each
% combination of \cc{treat} and \cc{year}) the results would have been the
% same. An argument in favor of the LS--means is that these figures
% better represent what one would expect on in an ``average year''.


% \subsection{Example: Warpbreaks}
% \label{sec:example:-warpbreaks}

% <<>>=
% summary( warpbreaks )
% head( warpbreaks, 4 )
% ftable(xtabs( ~ wool + tension, data=warpbreaks))
% @ %def

% <<echo=F>>=
%  opar <- par(mfrow = c(1, 2), oma = c(0, 0, 1.1, 0))
%      plot(breaks ~ tension, data = warpbreaks, col = "lightgray",
%           varwidth = TRUE, subset = wool == "A", main = "Wool A")
%      plot(breaks ~ tension, data = warpbreaks, col = "lightgray",
%           varwidth = TRUE, subset = wool == "B", main = "Wool B")
%      mtext("warpbreaks data", side = 3, outer = TRUE)
%      par(opar)
% @ %def

% <<>>=
% (warp.lm <- lm(breaks ~ wool + tension, data=warpbreaks))
% @ %def

% The fitted values are:
% <<>>=
% uni <- unique(warpbreaks[,2:3])
% prd <- cbind(breaks=predict(warp.lm, newdata=uni), uni); prd
% @ %def

% \subsection{The LS--means}
% \label{sec:lsmeans}

% We may be interested in making predictions of the number of breaks for
% each level of \cc{tension} for \emph{any} type or an \emph{average}
% type of \code{wool}.  The idea behind LS--means is
% to average the predictions above over the two
% wool types. These quantities are the \lsmeans\ for the effect
% \cc{tension}.

% This is done with:
% <<>>=
% LSmeans(warp.lm, effect="tension")
% @ %def

% The term \lsmeans\ comes from that these quantities are the same as
% the least squares main effects of \cc{tension} when data is balanced:
% <<>>=
% doBy::summaryBy(breaks ~ tension, data=warpbreaks, FUN=mean)
% @ %def
% When data is not balanced these quantities are in general not the same.




% \subsection{LS--means for models with interactions}
% \label{sec:models-with-inter}

% Consider a model with interaction:
% <<>>=
% warp.lm2 <- update(warp.lm, .~.+wool:tension)
% coef( summary( warp.lm2 ))
% @ %def

% In this case the contrast matrix becomes:
% <<>>=
% K2 <- LE_matrix(warp.lm2, effect="tension"); K2
% linest(warp.lm2, K=K2)
% @ %def


% For the sake of illustration we treat \cc{wool} as a random effect:

% <<>>=
% library(lme4)
% warp.mm <- lmer(breaks ~ tension + (1|wool), data=warpbreaks)
% LSmeans(warp.mm, effect="tension")
% @ %def

% Notice here that the estimates themselves are very similar to those
% above but the standard errors are much larger. This comes from that
% there that \code{wool} is treated as a random effect.

% <<>>=
% VarCorr(warp.mm)
% @ %def

%\usepackage{framed}
%\usepackage{comment}
%%\definecolor{shadecolor}{gray}{0.91}

%%\definecolor{darkred}{rgb}{.7,0,0}
%%\definecolor{midnightblue}{rgb}{0.098,0.098,0.439}

%% \DefineVerbatimEnvironment{Sinput}{Verbatim}{
%%   fontfamily=tt,
%%   %%fontseries=b,
%%   %% xleftmargin=2em,
%%   formatcom={\color{midnightblue}}
%% }
%% \DefineVerbatimEnvironment{Soutput}{Verbatim}{
%%   fontfamily=tt,
%%   %%fontseries=b,
%%   %% xleftmargin=2em,
%%   formatcom={\color{blue}}
%% }
%% \DefineVerbatimEnvironment{Scode}{Verbatim}{
%%   fontfamily=tt,
%%   %%fontseries=b,
%%   %% xleftmargin=2em,
%%   formatcom={\color{blue}}
%% }
%% 
%%\fvset{listparameters={\setlength{\topsep}{-2pt}}}
%\renewenvironment{Schunk}{\linespread{.90}}{}

%%\renewenvironment{Schunk}{\begin{shaded}\small}{\end{shaded}}









%% A special type of linear estimates is the so called least--squares
%% means (or LS--means). Other names for these estimates include
%% population means and marginal means. Notice that the \pkg{lsmeans} package
%% \cite{Lenth:2013} also provides computations of LS--means, see
%% \url{http://cran.r-project.org/web/packages/lsmeans/}.

%% <<echo=F>>=
%% simdat<-structure(list(treat = structure(c(1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L
%% ), .Label = c("t1", "t2"), class = "factor"), year = structure(c(1L,
%% 1L, 1L, 1L, 2L, 2L, 2L, 2L), .Label = c("1", "2"), class = "factor"),
%%     y = c(0.5, 1, 1.5, 3, 3, 4.5, 5, 5.5)), .Names = c("treat", "year",
%% "y"), row.names = c(NA, -8L), class = "data.frame")
%% @ %def

%% \subsection{LS--means on a simple example}
%% \label{sec:ls-means-simple}

%% Consider these simulated data, also shown in Fig.~\ref{fig:simdat2-fig}:
%% <<>>=
%% simdat
%% @ %def

%% <<simdat2-fig, fig.cap="LSmeans - simulated data.">>=
%% library(ggplot2)
%% qplot(treat, y, data=simdat, color=year)
%% @ %def


%% The LS--means under an additive model for the factor \cc{treat} is the predicted outcome for each level of \cc{treat} for an ``average year'':
%% <<>>=
%% msim <- lm(y ~ treat + year, data=simdat)
%% lsm <- LSmeans(msim, effect="treat")
%% lsm
%% @ %def


%% Notice that the population means are
%% <<>>=
%% summaryBy(y ~ treat, data=simdat, FUN=function(x) c(m=mean(x), s=sd(x)))
%% ## or aggregate(y ~ treat, data=simdat, FUN=function(x) c(m=mean(x), s=sd(x)))
%% @ %def

%% Had data been balanced (same number of observations for each
%% combination of \cc{treat} and \cc{year}) the results would have been the
%% same. An argument in favor of the LS--means is that these figures
%% better represent what one would expect on in an ``average year''.

%% An alternative is to think of  \cc{year} as a random
%% effect, for example:
%% <<eval=F>>=
%% library(lme4)
%% lmer( y ~ treat  + (1|year), data=simdat)
%% @ %def

%% In this case one would directly obtain $\EE(Y|\cc{treat})$ from the
%% model. However, there are at least two reasons why one may be hesitant
%% to consider such a random effects model.
%% \begin{itemize}
%% \item It is a ``never ending'' discussion whether \verb|year| should
%%   be treated as fixed or random. For \verb|year| to be a random
%%   effect, it should in principle come from a large population of
%%   possible years. This is certainly a dubious assumption for the
%%   years.
%% \item If we accept to think of year as a random effect, then the
%%   variance describing this random effect will be poorly determined
%%   (because there are only two years in the study).
%% \end{itemize}
 

% <<>>=
% warp.poi <- glm(breaks ~ wool + tension, family=poisson, data=warpbreaks)
% LSmeans(warp.poi, effect="tension", type="link")
% LSmeans(warp.poi, effect="tension", type="response")
% @ %def





%% SANITY CHECK
%% @
%% <<>>=
%% K <- LE_matrix(warp.poi, effect="tension")
%% doBy::esticon(warp.poi, K)
%% @ %def

% <<>>=
% warp.qpoi <- glm(breaks ~ wool + tension, family=quasipoisson, data=warpbreaks)
% LSmeans(warp.qpoi, effect="tension", type="link")
% LSmeans(warp.qpoi, effect="tension", type="response")
% @ %def


% For comparison with the linear model, we use identity link
% <<echo=F,results='hide'>>=
% warp.poi2 <- glm(breaks ~ wool + tension, family=poisson(link=identity),
%                  data=warpbreaks)
% LSmeans(warp.poi2, effect="tension", type="link")
% @ %def



%% SANITY CHECK
%% @
%% <<>>=
%% K <- LE_matrix(warp.poi2, effect="tension")
%% doBy::esticon(warp.poi2, K)
%% @ %def


% <<>>=
% warp.gam <- glm(breaks ~ wool + tension, family=Gamma(link=identity),
%                  data=warpbreaks)
% LSmeans(warp.gam, effect="tension", type="link")
% @ %def

%% SANITY CHECK
%% @
%% <<>>=
%% K <- LE_matrix(warp.gam, effect="tension")
%% doBy::esticon(warp.gam, K)
%% @ %def


% Notice that the linear estimates are practically the same as for the
% linear model, but the standard errors are smaller and hence the
% confidence intervals are narrower.

% An alternative is to fit a quasi Poisson ``model''

% <<>>=
% warp.poi3 <- glm(breaks ~ wool + tension, family=quasipoisson(link=identity),
%                  data=warpbreaks)
% LSmeans(warp.poi3, effect="tension")
% @ %def
